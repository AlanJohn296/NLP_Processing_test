{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ae7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference from https://realpython.com/nltk-nlp-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e41187",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"\"\"\n",
    "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "... And the first lesson of all was the basic trust that he could learn.\n",
    "... It's shocking to find how many people do not believe they can learn,\n",
    "... and how many more believe learning to be difficult.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e893b564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the example string into sentences\n",
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd86edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize the example string into words\n",
    "word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca157112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Filter stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042beed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02eb8077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_quote = word_tokenize(quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee123fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_list = []\n",
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff60fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76547486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming to reduce words to their root form\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f783c7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string_for_stemming' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m words = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mThe crew of the USS Discovery discovered many discoveries.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mDiscovering is what explorers do.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# word tokenizing the string before stemming\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m words = word_tokenize(\u001b[43mstring_for_stemming\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'string_for_stemming' is not defined"
     ]
    }
   ],
   "source": [
    "words = \"\"\"\n",
    "... The crew of the USS Discovery discovered many discoveries.\n",
    "... Discovering is what explorers do.\"\"\"\n",
    "# word tokenizing the string before stemming\n",
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String in words tokenization: ['The', 'crew', 'of', 'the', 'USS', 'Discovery', 'discovered', 'many', 'discoveries', '.', 'Discovering', 'is', 'what', 'explorers', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"String in words tokenization:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f86a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words of the tokenizied words ['the', 'crew', 'of', 'the', 'uss', 'discoveri', 'discov', 'mani', 'discoveri', '.', 'discov', 'is', 'what', 'explor', 'do', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words of the tokenizied words\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da2d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging (Parts of Speech tagging)\n",
    "sagan_quote = \"\"\"If you wish to make an apple pie from scratch, you must first invent the universe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_sagan_quote = word_tokenize(sagan_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44071d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'wish', 'to', 'make', 'an', 'apple', 'pie', 'from', 'scratch', ',', 'you', 'must', 'first', 'invent', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words_in_sagan_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f96e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging of the words in the quote\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apple', 'NN'),\n",
       " ('pie', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('scratch', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('first', 'VB'),\n",
       " ('invent', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"POS tagging of the words in the quote\")\n",
    "nltk.pos_tag(words_in_sagan_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a870f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "# TO get a list of tags and their meanings\n",
    "nltk.help.upenn_tagset(\"VB\")\n",
    "nltk.help.upenn_tagset(\"VBP\")\n",
    "nltk.help.upenn_tagset(\"NN\")\n",
    "nltk.help.upenn_tagset(\"IN\")\n",
    "nltk.help.upenn_tagset(\"PRP\")\n",
    "nltk.help.upenn_tagset(\"DT\")\n",
    "nltk.help.upenn_tagset(\"MD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a6a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'T\", 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('brillig', 'VBN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('slithy', 'JJ'),\n",
       " ('toves', 'NNS'),\n",
       " ('did', 'VBD'),\n",
       " ('gyre', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('gimble', 'JJ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wabe', 'NN'),\n",
       " (':', ':'),\n",
       " ('all', 'DT'),\n",
       " ('mimsy', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('borogoves', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('mome', 'JJ'),\n",
       " ('raths', 'NNS'),\n",
       " ('outgrabe', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jabberwocky_excerpt = \"'Twas brillig, and the slithy toves did gyre and gimble in the wabe: all mimsy were the borogoves, and the mome raths outgrabe.\"\n",
    "\n",
    "words_in_jabberwocky = word_tokenize(jabberwocky_excerpt)\n",
    "\n",
    "nltk.pos_tag(words_in_jabberwocky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f6a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizing to reduce words to their base form with a complete English word.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5d670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The friends of DeSoto love scarves'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves\"\n",
    "lemmatizer.lemmatize(string_for_lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc237cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea62d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c7083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
